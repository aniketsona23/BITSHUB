# -*- coding: utf-8 -*-
"""OOPs RAG ChatBot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JwFgtgpgwu8Bb-WqDa-n11h9mVmscUN-
"""

# Install necessary libraries
!pip install PyPDF2 openai faiss-cpu
!pip install --upgrade openai
# Import necessary libraries
import PyPDF2
import openai
import numpy as np
import faiss
import os

from openai import OpenAI
os.environ["OPENAI_API_KEY"] = "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"  # Replace with your OpenAI API key

client = OpenAI()
# Set OpenAI API Key


# Function to extract text from a PDF file
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, "rb") as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

# Function to split text into smaller chunks
def split_text_into_chunks(text, chunk_size=500, overlap=50):
    words = text.split()
    chunks = []
    for i in range(0, len(words), chunk_size - overlap):
        chunk = " ".join(words[i:i + chunk_size])
        chunks.append(chunk)
    return chunks

# Function to generate embeddings for chunks using the latest OpenAI SDK
def generate_embeddings(chunks):
    embeddings = []
    for chunk in chunks:
        response = client.embeddings.create(
            input=chunk,
            model="text-embedding-ada-002"  # OpenAI's embedding model
        )
        # Access the data attribute directly
        embedding = response.data[0].embedding
        embeddings.append(embedding)
    return embeddings

# Function to save embeddings using FAISS
def save_embeddings_to_faiss(embeddings, output_path="faiss_index"):
    dimension = len(embeddings[0])
    index = faiss.IndexFlatL2(dimension)
    faiss_index = faiss.IndexIDMap(index)
    ids = list(range(len(embeddings)))
    faiss_index.add_with_ids(np.array(embeddings).astype("float32"), np.array(ids))
    faiss.write_index(faiss_index, output_path)
    print(f"FAISS index saved at {output_path}")

# Main function
def process_pdf_to_embeddings(pdf_path, faiss_output_path="faiss_index"):
    text = extract_text_from_pdf(pdf_path)
    chunks = split_text_into_chunks(text)
    embeddings = generate_embeddings(chunks)
    save_embeddings_to_faiss(embeddings, faiss_output_path)
    return chunks

# Example usage
pdf_path = "/content/CS F213 Handout_1 2024 25 (23 files merged).pdf"  # Replace with the path to your PDF file
process_pdf_to_embeddings(pdf_path, "faiss_index")

# Query the FAISS index
def query_faiss_index(query, faiss_index_path="/content/faiss_index", top_k=3):
    # Load FAISS index
    index = faiss.read_index(faiss_index_path)

    # Generate embedding for the query
    response = client.embeddings.create(
        input=query,
        model="text-embedding-ada-002"
    )
    query_embedding = np.array(response.data[0].embedding).astype("float32").reshape(1, -1)

    # Search in the FAISS index
    distances, indices = index.search(query_embedding, k=top_k)
    return distances, indices

# Generate a response using OpenAI GPT
def generate_response(query, relevant_chunks):
    context = " ".join(relevant_chunks)
    prompt = f"Context: {context}\n\nQuery: {query}\nAnswer:"
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[ {"role": "system", "content": "You are a concise assistant. Provide brief and to-the-point answers."},
         {"role": "user", "content": prompt}],
        max_tokens=200
    )
    return response.choices[0].message.content

# Process a PDF and save embeddings
def process_pdf_to_embeddings(pdf_path, faiss_output_path="faiss_index"):
    text = extract_text_from_pdf(pdf_path)
    chunks = split_text_into_chunks(text)
    embeddings = generate_embeddings(chunks)
    save_embeddings_to_faiss(embeddings, faiss_output_path)
    return chunks  # Return chunks for mapping indices to text

# Test the system
def test_system(query, faiss_index_path, text_chunks, top_k=5):
    distances, indices = query_faiss_index(query, faiss_index_path, top_k=top_k)
    relevant_chunks = [text_chunks[i] for i in indices[0] if i != -1]
    answer = generate_response(query, relevant_chunks)
    return answer

# Example usage
pdf_path = "/content/CS F213 Handout_1 2024 25 (23 files merged).pdf"  # Path to your PDF
faiss_index_path = "/content/faiss_index"

# Step 1: Process PDF and create embeddings
text_chunks = process_pdf_to_embeddings(pdf_path, faiss_index_path)

# Step 2: Test the system with a query
query = "Who is the instructor in charge and what are evaluative components."
answer = test_system(query, faiss_index_path, text_chunks)
print("Generated Answer:", answer)

# Step 2: Test the system with a query
query = "Which of the following classes can override the equals() method from the Object class? No class can override equals(). Only abstract classes. All classes. Only final classes"
answer = test_system(query, faiss_index_path, text_chunks)
print("Generated Answer:", answer)